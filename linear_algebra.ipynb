{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_add(v, w):\n",
    "    \"\"\"adds two vectors componentwise\"\"\"\n",
    "    return [v_i + w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"subtracts two vectors componentwise\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    return reduce(vector_add, vectors)\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"compute the vector whose i-th element is the mean of the\n",
    "    i-th elements of the input vectors\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "    return math.sqrt(squared_distance(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [1, 2, 3]\n",
      "y: [4, 5, 6]\n",
      "dot product:\t32\n",
      "sum squares x:\t14\n",
      "magnitude x:\t3.74\n"
     ]
    }
   ],
   "source": [
    "x = [1,2,3]\n",
    "y = [4,5,6]\n",
    "print 'x: {}'.format(x)\n",
    "print 'y: {}'.format(y)\n",
    "print 'dot product:\\t{}'.format(dot(x,y))\n",
    "print 'sum squares x:\\t{}'.format(sum_of_squares(x))\n",
    "print 'magnitude x:\\t{:.3}'.format(magnitude(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape(A):\n",
    "    num_rows = len(A)\n",
    "    num_cols = len(A[0]) if A else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "def get_row(A, i):\n",
    "    return A[i]\n",
    "\n",
    "def get_column(A, j):\n",
    "    return [A_i[j] for A_i in A]\n",
    "\n",
    "def make_matrix(num_rows, num_cols, entry_fn):\n",
    "    \"\"\"returns a num_rows x num_cols matrix\n",
    "    whose (i,j)-th entry is entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i, j) for j in range(num_cols)]\n",
    "            for i in range(num_rows)]\n",
    "\n",
    "def is_diagonal(i, j):\n",
    "    \"\"\"1's on the 'diagonal', 0's everywhere else\"\"\"\n",
    "    return 1 if i == j else 0\n",
    "\n",
    "def matrix_add(A, B):\n",
    "    if shape(A) != shape(B):\n",
    "        raise ArithmeticError(\n",
    "            \"cannot add matrices with different shapes\")\n",
    "\n",
    "    num_rows, num_cols = shape(A)\n",
    "    def entry_fn(i, j): return A[i][j] + B[i][j]\n",
    "\n",
    "    return make_matrix(num_rows, num_cols, entry_fn)\n",
    "\n",
    "def matrix_multiply(A, B):\n",
    "    \"\"\"If A is a n x m matrix and B is a m x l matrix,\n",
    "       then AB is defined and has the dimension n x l\"\"\"\n",
    "    Ar, Ac = shape(A)\n",
    "    Br, Bc = shape(B)\n",
    "    \n",
    "    if not Ac == Br:\n",
    "        raise ArithmeticError('Incorrect format for matrices')\n",
    "        \n",
    "    results = [[sum(a * b for a, b in zip(A_row, B_col)) \n",
    "              for B_col in zip(*B)] for A_row in A]\n",
    "    \n",
    "    for result in results:\n",
    "        print result\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[7, 8, 9]\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "A = [[1,2,3],\n",
    "     [7,8,9],\n",
    "     [0,0,1]]\n",
    "\n",
    "B = [[4,5,8],\n",
    "     [6,8,9],\n",
    "     [7,6,1]]\n",
    "\n",
    "D = make_matrix(3,3, is_diagonal)\n",
    "mp = matrix_multiply(A, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def de_mean_matrix(A):\n",
    "    \"\"\"returns the result of subtracting \n",
    "    from every value in A the mean\n",
    "    value of its column. The resulting matrix \n",
    "    has mean 0 in every column\"\"\"\n",
    "    nr, nc = shape(A)\n",
    "    column_means, _ = scale(A)\n",
    "    return make_matrix(nr, nc, lambda i, j: A[i][j] - column_means[j])\n",
    "\n",
    "def direction(w):\n",
    "    mag = magnitude(w)\n",
    "    return [w_i / mag for w_i in w]\n",
    "\n",
    "def directional_variance_i(x_i, w):\n",
    "    \"\"\"the variance of the row x_i in the direction w\"\"\"\n",
    "    return dot(x_i, direction(w)) ** 2\n",
    "\n",
    "def directional_variance(X, w):\n",
    "    \"\"\"the variance of the data in the direction w\"\"\"\n",
    "    return sum(directional_variance_i(x_i, w) for x_i in X)\n",
    "\n",
    "def directional_variance_gradient_i(x_i, w):\n",
    "    \"\"\"the contribution of row x_i to the gradient of\n",
    "    the direction-w variance\"\"\"\n",
    "    projection_length = dot(x_i, direction(w))\n",
    "    return [2 * projection_length * x_ij for x_ij in x_i]\n",
    "\n",
    "def directional_variance_gradient(X, w):\n",
    "    return vector_sum(\n",
    "        directional_variance_gradient_i(x_i,w) for x_i in X)\n",
    "\n",
    "def first_principal_component(X):\n",
    "    guess = [1 for _ in X[0]]\n",
    "    unscaled_maximizer = maximize_batch(\n",
    "        partial(directional_variance, X),           # now a func of w\n",
    "        partial(directional_variance_gradient, X),  # now a func of w\n",
    "        guess)\n",
    "    return direction(unscaled_maximizer)\n",
    "\n",
    "def first_principal_component_sgd(X):\n",
    "    guess = [1 for _ in X[0]]\n",
    "    unscaled_maximizer = maximize_stochastic(\n",
    "        lambda x, _, w: directional_variance_i(x, w),\n",
    "        lambda x, _, w: directional_variance_gradient_i(x, w),\n",
    "        X, [None for _ in X], guess)\n",
    "    return direction(unscaled_maximizer)\n",
    "\n",
    "def project(v, w):\n",
    "    \"\"\"return the projection of v onto w\"\"\"\n",
    "    coefficient = dot(v, w)\n",
    "    return scalar_multiply(coefficient, w)\n",
    "\n",
    "def remove_projection_from_vector(v, w):\n",
    "    \"\"\"projects v onto w and subtracts the result from v\"\"\"\n",
    "    return vector_subtract(v, project(v, w))\n",
    "\n",
    "def remove_projection(X, w):\n",
    "    \"\"\"for each row of X\n",
    "    projects the row onto w, and subtracts the result from the row\"\"\"\n",
    "    return [remove_projection_from_vector(x_i, w) for x_i in X]\n",
    "\n",
    "def principal_component_analysis(X, num_components):\n",
    "    components = []\n",
    "    for _ in range(num_components):\n",
    "        component = first_principal_component(X)\n",
    "        components.append(component)\n",
    "        X = remove_projection(X, component)\n",
    "\n",
    "    return components\n",
    "\n",
    "def transform_vector(v, components):\n",
    "    return [dot(v, w) for w in components]\n",
    "\n",
    "def transform(X, components):\n",
    "    return [transform_vector(x_i, components) for x_i in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "def derivative_estimate(function, x, h=0.00001):\n",
    "    # Calculate mean of h above and below for extra accuracy!\n",
    "    dq_high = difference_quotient(function, x, h)\n",
    "    dq_low = difference_quotient(function, x, -h)\n",
    "    return (dq_high + dq_low) / 2\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    # add h to just the i-th element of v (input vector)\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def safe(f): # Safe meaning will give default value on f error\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') \n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func_x_y(x,y):\n",
    "    return x**2 - (3 * np.sqrt(y))\n",
    "\n",
    "xs = np.linspace(-50, 50, 100)\n",
    "ys = np.linspace(20, -20, 100)\n",
    "zs = [func_x_y(x,y) for x in xs for y in ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimization/maximazation - batch\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta \n",
    "       that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "    theta = theta_0               # set theta to initial value\n",
    "    target_fn = safe(target_fn)   # safe version of target_fn\n",
    "    value = target_fn(theta)      # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)\n",
    "\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0                             # initial guess\n",
    "    alpha = alpha_0                             # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\")   # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "\n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "\n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "\n",
    "    return min_theta\n",
    "\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
